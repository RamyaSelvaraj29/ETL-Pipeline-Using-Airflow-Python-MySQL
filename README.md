# ETL-Pipeline-Using-Airflow-Python-MySQL
## Business Problem
Retail analytics teams often receive customer and order data as separate file exports from transactional systems. Analysts spend hours every week manually cleaning files, joining tables in spreadsheets, and re-creating the same metrics such as daily revenue, order counts, and customer activity.

This manual process is:
- **Error prone** - joins break when columns change or files are missing  
- **Slow** - every new report requires repetitive data preparation  
- **Hard to scale** - as data grows, spreadsheets and ad-hoc scripts don’t keep up

## Overview 
This project implements an end-to-end ETL pipeline using Apache Airflow, Python, and MySQL, designed to automate data ingestion, validation, transformation, and loading. The pipeline standardizes multiple raw CSV inputs into a structured warehouse layer and produces a final analytics ready table for downstream dashboards and analysis. It demonstrates a scalable, maintainable, and fully automated data workflow that improves data reliability, reduces manual intervention, and supports business intelligence and machine-learning use cases.

## Technology Stack 
- **Orchestration:** ![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-017CEE?style=flat&logo=apacheairflow&logoColor=white)
- **Database:** ![MySQL](https://img.shields.io/badge/MySQL-4479A1?style=flat&logo=mysql&logoColor=white)
- **Containerization:** ![Docker](https://img.shields.io/badge/Docker-2496ED?style=flat&logo=docker&logoColor=white)
- **Data Processing/Transformation:** ![Python](https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white) ![Pandas](https://img.shields.io/badge/Pandas-150458?style=flat&logo=pandas&logoColor=white)

## Project workflow
- **Raw Data Ingestion -**
The ETL pipeline begins with three raw CSV files customers, orders, and order items, placed in Airflow’s shared data directory `(/opt/airflow/data)`. This directory serves as a controlled landing area for all input datasets, ensuring every pipeline run starts from a single, consistent source. Centralizing the raw data guarantees reproducibility, avoids path inconsistencies, and provides a reliable entry point for all downstream processing.
- **File Availability Validation -**
Before any processing occurs, the DAG performs a validation check to confirm that all required files are present. Using a `BranchPythonOperator`, the workflow stops execution if any file is missing. This prevents partial or inconsistent ETL runs and ensures that all downstream components receive complete and accurate inputs. Validating file availability upfront protects the pipeline from failures later in the process.
 - **Staging Layer -**
Once all input files are validated, the pipeline loads the CSV files into dedicated MySQL staging tables (`stg_customers`, `stg_orders`, `stg_order_items`). `PythonOperator` handle parsing, datatype standardization, null value handling, and full table refresh for each run. The staging layer acts as an intermediate buffer between raw data and the warehouse, providing a clean, structured environment for transformations. It isolates raw file formats from business logic, ensures consistent behavior during reruns, and lays the foundation for building structured, reliable datasets.
- **Warehouse Transformation -**
After staging is loaded, Airflow constructs the warehouse tables using Python-based transformations. All warehouse tables use UPSERT logic, ensuring that repeated pipeline runs produce consistent and reliable results across reruns.
   - **`dim_customer`** - Is generated by combining customer attributes with aggregated order behavior.
   - **`fact_order`** - Is built from order level data combined with item-level summaries such as revenue and number of items.
   - **`fact_order_item`** - Contains product and seller level details.
- **Final Analytics Table -**
The pipeline then creates the final_orders table, a consolidated dataset that merges customer, order, and item information. This consolidated table simplifies analysis, reporting, and visualization, as all relevant information is readily accessible in one place. By combining the warehouse tables into a single output, the pipeline eliminates the need for manual joins or additional transformations during downstream analysis.
- **Data Quality Validation -**
Before the DAG completes, a validation step checks the final dataset for missing values, invalid dates, logical inconsistencies, or empty outputs. If issues are detected, the DAG fails early to prevent corrupted data from flowing into consuming systems. This automated data validation feature  significantly improves reliability.

## Airflow DAG
![ETL Pipeline Workflow](https://github.com/RamyaSelvaraj29/ETL-Pipeline-Using-Airflow-Python-MySQL/blob/main/DAG%20Graph.png)

## Conclusion
This ETL pipeline demonstrates how Airflow, Python, and MySQL can be combined to build a robust and automated data workflow. By validating raw inputs, organizing data into structured layers, and enforcing data quality checks, the pipeline ensures consistent, reliable, and analytics-ready output.

While this project is intentionally designed for clarity and reproducibility using local CSV ingestion and full refresh logic it also forms a solid foundation for more advanced enhancements. In real enterprise environments, the same architecture can be extended to pull data from cloud storage systems like AWS S3 or Google Cloud Storage, and evolve into an incremental loading pattern that processes only new or updated records instead of refreshing entire tables. These improvements naturally build on the existing workflow and demonstrate how this project can scale as data volume and business requirements grow.
